{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import string\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Download stopwords and punkt if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_df = pd.read_csv(\"data/Mname&Captions.csv\")\n",
    "music_folder = 'data/music'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_df = caption_df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_p(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Function to stem each word in the text\n",
    "def stem_text(text):\n",
    "    words = word_tokenize(text)\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    return ' '.join(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocess(text):\n",
    "    text = text_p(text)\n",
    "    text = remove_stop_words(text)\n",
    "    text = stem_text(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What did I done for text_preprocess\n",
    "1. Lowercasing\n",
    "2. Removing Punctuation\n",
    "3. Removing Stop Words\n",
    "4. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_df[\"Preprocessed_Text\"] = caption_df['caption'].apply(text_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embedding using GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe model\n",
    "def load_glove_model(file_path):\n",
    "    print(\"Loading GloVe Model...\")\n",
    "    glove_model = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.split()\n",
    "            word = parts[0]\n",
    "            vector = np.array(parts[1:], dtype='float32')\n",
    "            glove_model[word] = vector\n",
    "    print(f\"Done. {len(glove_model)} words loaded!\")\n",
    "    return glove_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model = load_glove_model(\"data\\glove.6B.50d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get sentence embedding\n",
    "def get_sentence_embedding(sentence, model):\n",
    "    words = sentence.split()\n",
    "    word_vectors = [model[word] for word in words if word in model]\n",
    "    if not word_vectors:  # If none of the words are in the model, return a zero vector\n",
    "        return np.zeros(next(iter(model.values())).shape)\n",
    "    return np.mean(word_vectors, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_df['embedding'] = caption_df['Preprocessed_Text'].apply(lambda x: get_sentence_embedding(x, glove_model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "ipd.Audio(f\"data/music/{caption_df['audio'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Period is the time interval before the next peak\n",
    "# Frequency is interval correlated with Period e.g. high period == lower frequency and vise varas {f = 1 / T}\n",
    "\n",
    "# Amplitude is the length of the peak or valley  A\n",
    "# face shift the wave to right or to left \n",
    "# Simple wave y(t) = A sin(2pi ft = face(greek latter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two fundamental function of sound wave are Frequency and Amplitude and they are related to pitch and loudness i.e.\n",
    "# Pitch is not phelay Feature of the sound \n",
    "# Frequency & Pitch :- Higher frequency == Higher pitch\n",
    "# Amplitude & Loudness :- larger amplitude are louder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "As all the sound wave are Analog and is conutunels and we and not stroe it in digital form with highest relustion so we need\n",
    "Analog digital conversion (ADC) and staps for ADC\n",
    "1. Signal sampled at uniform time intervals (Sample rate = 44,100 Hz)\n",
    "2. Amplitude quantized with limited number of bits (Bit depth = 16 bits/channel)\n",
    "at each Sample will project the value of amplitude to the closest quantized bit at each interval\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Real sound wave are really messy so for undertding the sound wave will use \n",
    "Fourier Transform :- Decompose complex periodic sound into sum of sine waves oscillating at different frequencies.\n",
    "\n",
    "Complex sound (s) = A1 sin(2pi f1t = face(greek latter)1 + A2 sin(2pi f2t = face(greek latter)2\n",
    "and value of amplitude will dicecd how much a particali sin function will conterbut to the final complex sound. i.e. Bigger the Amplitude more it will efacte the Complex sound\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FFT (Fourier Transform) will give a power spectrum wiche will give us a snap shot of the sound wave\n",
    "by giving Magnitude / Power of Amplitude @ Y-axis and Frequency @ X-axis of the whole sound wave. \n",
    "Becuse of this we will miss the info about the Time Domain.\n",
    "FFT will From Time domain to Frequency Domain\n",
    "There is no info about time\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Solution Short Time Fourier Transform (STFT)\n",
    "- Computes several FFT at different intervals\n",
    "- To Preserves time info\n",
    "- Will do at a Fixed frame size (e.g. 2048 samples) at ocens and then move on and do FFT at same Samples\n",
    "- Gives us a Spectrogram (Time + Frequency + magnitude)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Mel Frequency Cepstral Coefficients (MFCCs) ## This will be the bsice diffrent b/w if  Frequency and Amplitude anre same for pieno and gitare it help for making a DIffrente\n",
    "- Capture timbral / textural aspects of the sound\n",
    "- Frequency domain feature \n",
    "- Approximate hume auditory system\n",
    "- 13 to 40 coefficients\n",
    "- Calculated at each frame\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sound1 = f\"data/music/{caption_df['audio'][0]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa , librosa.display\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def audio(sound):\n",
    "#     signal, sample_rate = librosa.load(sound, sr=22050) # signal is a nparry with sr * T values -> 22050 * 4 and the values are the Ampletued at each sr\n",
    "#     # FFT -> Power Spectrum\n",
    "#     fft = np.fft.fft(signal)\n",
    "#     magnitude = np.abs(fft)\n",
    "#     frequency = np.linspace(0, sample_rate, len(magnitude))\n",
    "    \n",
    "#     # STFT\n",
    "#     n_FFT = 2048 # Window for FFT\n",
    "#     hop_length = 512 # Shift rate\n",
    "#     STFT = librosa.core.stft(signal, hop_length=hop_length, n_fft=n_FFT)\n",
    "#     spectrogram = np.abs(STFT) \n",
    "#     log_spectrogram = librosa.amplitude_to_db(spectrogram)\n",
    "\n",
    "#     # Compute MFCCs\n",
    "#     MFFCs = librosa.feature.mfcc(y=signal, sr=sample_rate, n_fft=n_FFT, hop_length=hop_lenght, n_mfcc=13)\n",
    "\n",
    "#     return log_spectrogram, MFFCs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some useful Fetures for Text to Audio\n",
    "1. Spectral Features\n",
    "   1. Spectrogram (Frequency spectrum over time.)\n",
    "   2. Mel-Spectrogram (How do humans hear)\n",
    "2. Cepstral Features\n",
    "   1. Mel-Frequency Cepstral Coefficients (MFCCs): SFTF (Useful for Timbral aspects)\n",
    "3. Chroma features (12 differnt pitch classes (like C, C#, D, etc.)) and Harmonic content\n",
    "4. Rhythmic Feactures\n",
    "   1. Tempo : Speed beats occur in the audio\n",
    "   2. Beat and Onset Detection \n",
    "5. Tonnetz (Tonal Centroid Features)\n",
    "   1. Tonnetz: Describes the harmonic relations in music, like tonality and chordal structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_audio(sound_path):\n",
    "    sound_path = f\"data/music/{sound_path}\"\n",
    "    # Load the audio file\n",
    "    signal, sample_rate = librosa.load(sound_path, sr=22050)\n",
    "\n",
    "    # Short-Time Fourier Transform (STFT)\n",
    "    n_FFT = 2048  # Window size for FFT\n",
    "    hop_length = 512  # Hop length for FFT\n",
    "    STFT = librosa.core.stft(signal, hop_length=hop_length, n_fft=n_FFT)\n",
    "    spectrogram = np.abs(STFT)\n",
    "    log_spectrogram = librosa.amplitude_to_db(spectrogram)\n",
    "\n",
    "    # Compute Mel-Frequency Cepstral Coefficients (MFCCs)\n",
    "    MFFCs = librosa.feature.mfcc(y=signal, sr=sample_rate, n_fft=n_FFT, hop_length=hop_length, n_mfcc=13)\n",
    "    MFFCs = MFFCs.T # Tranpose the MFFCs\n",
    "\n",
    "    return log_spectrogram, MFFCs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# librosa.display.waveshow(signal, sr = sample_rate)\n",
    "# plt.xlabel(\"Time\")\n",
    "# plt.ylabel(\"Amplitude\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # FFT -> Power Spectrum\n",
    "# fft = np.fft.fft(signal)\n",
    "# magnitude = np.abs(fft)\n",
    "# frequency = np.linspace(0, sample_rate, len(magnitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# left_frequency = frequency[:int(len(frequency)/2)]\n",
    "# left_magnitude = magnitude[:int(len(frequency)/2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ploting Power Spectrum\n",
    "# plt.plot(left_frequency, left_magnitude)\n",
    "# plt.xlabel(\"Frequency\")\n",
    "# plt.ylabel(\"Magnitude\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # STFT\n",
    "# n_FFT = 2048 # Window for FFT\n",
    "# hop_lenght = 512 # Shift rate\n",
    "# STFT = librosa.core.stft(signal, hop_length=hop_lenght, n_fft=n_FFT)\n",
    "# spectrogram = np.abs(STFT) \n",
    "# log_spectrogram = librosa.amplitude_to_db(spectrogram)\n",
    "# librosa.display.specshow(log_spectrogram, sr=sample_rate, hop_length=hop_lenght)\n",
    "\n",
    "# plt.xlabel(\"Time\")\n",
    "# plt.ylabel(\"Frequency\")\n",
    "# plt.colorbar()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute MFCCs\n",
    "# MFFCs = librosa.feature.mfcc(y=signal, sr=sample_rate, n_fft=n_FFT, hop_length=hop_lenght, n_mfcc=13)\n",
    "\n",
    "# # Display the MFCCs\n",
    "# librosa.display.specshow(MFFCs, sr=sample_rate, hop_length=hop_lenght, x_axis='time')\n",
    "\n",
    "# plt.xlabel(\"Time\")\n",
    "# plt.ylabel(\"MFCCs\")\n",
    "# plt.colorbar()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_df[[\"Log_Spectrogram\", \"MFFCs\"]] = caption_df[\"audio\"].apply(lambda x: pd.Series(process_audio(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tiny-tempo-5kRMDcSD-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
